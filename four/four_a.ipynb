{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:41:42.679294Z","iopub.execute_input":"2025-04-09T13:41:42.679532Z","iopub.status.idle":"2025-04-09T13:41:42.897924Z","shell.execute_reply.started":"2025-04-09T13:41:42.679507Z","shell.execute_reply":"2025-04-09T13:41:42.897144Z"}},"outputs":[{"name":"stdout","text":"Wed Apr  9 13:41:42 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   45C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   34C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile vector_add.cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n#include <math.h> // For fabs in verification\n\n// Simple CUDA Error Handling Macro\n#define CHECK_CUDA_ERROR(err) \\\n    if (err != cudaSuccess) { \\\n        fprintf(stderr, \"CUDA Error at %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n        exit(EXIT_FAILURE); \\\n    }\n\n// CUDA Kernel for Vector Addition\n__global__ void vectorAddKernel(const float *a, const float *b, float *c, int n) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < n) {\n        c[index] = a[index] + b[index];\n    }\n}\n\nint main() {\n    int n = 1 << 24; // ~16.7 million elements (a large vector)\n    size_t size = n * sizeof(float);\n    printf(\"Vector Addition (CUDA)\\nVector size: %d elements (%.2f MB)\\n\", n, (float)size / (1024*1024));\n\n    // Host memory\n    float *h_a = (float*)malloc(size);\n    float *h_b = (float*)malloc(size);\n    float *h_c = (float*)malloc(size);\n    if (!h_a || !h_b || !h_c) {\n        fprintf(stderr, \"Failed to allocate host vectors!\\n\"); return EXIT_FAILURE;\n    }\n\n    // Initialize host vectors\n    for (int i = 0; i < n; ++i) {\n        h_a[i] = (float)i;\n        h_b[i] = (float)i * 2.0f;\n    }\n\n    // Device memory\n    float *d_a = NULL, *d_b = NULL, *d_c = NULL;\n    printf(\"Allocating %.2f MB on device...\\n\", 3.0f * size / (1024*1024));\n    CHECK_CUDA_ERROR(cudaMalloc(&d_a, size));\n    CHECK_CUDA_ERROR(cudaMalloc(&d_b, size));\n    CHECK_CUDA_ERROR(cudaMalloc(&d_c, size));\n\n    // Copy data Host -> Device\n    printf(\"Copying data to device...\\n\");\n    CHECK_CUDA_ERROR(cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice));\n    CHECK_CUDA_ERROR(cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice));\n\n    // Kernel launch configuration\n    int blockSize = 256;\n    int gridSize = (n + blockSize - 1) / blockSize;\n    printf(\"Launching kernel (Grid: %d blocks, Block: %d threads)...\\n\", gridSize, blockSize);\n\n    // Launch kernel\n    vectorAddKernel<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\n    CHECK_CUDA_ERROR(cudaPeekAtLastError()); // Check for launch errors\n    CHECK_CUDA_ERROR(cudaDeviceSynchronize()); // Wait for kernel completion & check run errors\n    printf(\"Kernel finished.\\n\");\n\n    // Copy data Device -> Host\n    printf(\"Copying result back to host...\\n\");\n    CHECK_CUDA_ERROR(cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost));\n\n    // Verification (simple check)\n    printf(\"Verifying result...\\n\");\n    bool success = true;\n    float tolerance = 1e-5f;\n    if (fabs(h_c[0] - (h_a[0] + h_b[0])) > tolerance ||\n        fabs(h_c[n-1] - (h_a[n-1] + h_b[n-1])) > tolerance) {\n        success = false;\n        printf(\"Mismatch detected: h_c[0]=%.f vs %.f, h_c[n-1]=%.f vs %.f\\n\",\n               h_c[0], h_a[0] + h_b[0], h_c[n-1], h_a[n-1] + h_b[n-1]);\n    }\n\n    printf(\"Verification: %s\\n\", success ? \"Successful!\" : \"FAILED!\");\n\n    // Cleanup\n    printf(\"Freeing memory...\\n\");\n    CHECK_CUDA_ERROR(cudaFree(d_a));\n    CHECK_CUDA_ERROR(cudaFree(d_b));\n    CHECK_CUDA_ERROR(cudaFree(d_c));\n    free(h_a);\n    free(h_b);\n    free(h_c);\n\n    printf(\"Vector addition complete.\\n\");\n    return EXIT_SUCCESS;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:42:40.438224Z","iopub.execute_input":"2025-04-09T13:42:40.438537Z","iopub.status.idle":"2025-04-09T13:42:40.445371Z","shell.execute_reply.started":"2025-04-09T13:42:40.438514Z","shell.execute_reply":"2025-04-09T13:42:40.444386Z"}},"outputs":[{"name":"stdout","text":"Writing vector_add.cu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!nvcc vector_add.cu -o vector_add","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:42:59.702574Z","iopub.execute_input":"2025-04-09T13:42:59.702917Z","iopub.status.idle":"2025-04-09T13:43:02.859265Z","shell.execute_reply.started":"2025-04-09T13:42:59.702887Z","shell.execute_reply":"2025-04-09T13:43:02.858283Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!./vector_add","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T13:43:16.882255Z","iopub.execute_input":"2025-04-09T13:43:16.882574Z","iopub.status.idle":"2025-04-09T13:43:17.625705Z","shell.execute_reply.started":"2025-04-09T13:43:16.882545Z","shell.execute_reply":"2025-04-09T13:43:17.624676Z"}},"outputs":[{"name":"stdout","text":"Vector Addition (CUDA)\nVector size: 16777216 elements (64.00 MB)\nAllocating 192.00 MB on device...\nCopying data to device...\nLaunching kernel (Grid: 65536 blocks, Block: 256 threads)...\nKernel finished.\nCopying result back to host...\nVerifying result...\nVerification: Successful!\nFreeing memory...\nVector addition complete.\n","output_type":"stream"}],"execution_count":5}]}